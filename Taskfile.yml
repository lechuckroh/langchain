# https://taskfile.dev

version: "3"

tasks:
  venv:
    summary: "install venv"
    cmds:
      - python3 -m venv venv

  install:
    summary: "install dependency"
    cmds:
      - pip3 install -r requirements.txt

  freeze:
    cmds:
      - pip3 freeze > requirements.txt

  pull-llama-korean:
    summary: "Pull Llama3.2-korean model"
    cmds:
      - wget --output-document ./models/llma3.2-korean/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf https://huggingface.co/Bllossom/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M/resolve/main/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf?download=true
      - ollama create llama3.2-korean -f ./models/llma3.2-korean/Modelfile

  llama-korean:
    summary: "Run Llama3.2-korean model"
    cmds:
      - ollama run llama3.2-korean

  open-webui:
    summary: "Start Open WebUI server"
    cmds:
      - docker run -d --rm -p 3000:8080 -e WEBUI_AUTH=False --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main

  run:
    cmds:
      - python3 src/llama.py

  run-chat:
    cmds:
      - python3 src/llama_chat.py

  run-streaming:
    cmds:
      - python3 src/llama_streaming.py

  run-few-shot:
    cmds:
      - python3 src/llama_few_shot.py

  run-output-parser:
    cmds:
      - python3 src/llama_output_parser.py

  run-pydantic-output-parser:
    cmds:
      - python3 src/llama_pydantic_output_parser.py
